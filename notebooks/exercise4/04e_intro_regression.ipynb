{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4: Introduction to Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aufgaben:\n",
    "\n",
    "* Programmierung OLS, \n",
    "* Programmierung Diagnostics: Bootstrap CI, R²\n",
    "* Generierung Zufallsdaten nach Muster, Check Funktionalität, Plot daten und Regreesionsgerade\n",
    "* Datensatz Student Performance (Preprocessing [!], Übergang zu statsmodels\n",
    "* Exeriments Predictors\n",
    "* Analyse Prediktoren\n",
    "* Residual Plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'statsmodels'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-bce3ea2f0fed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'statsmodels'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Brief Introduction to Statsmodels\n",
    "\n",
    "Next to sklearn, statsmodels is probably the most popular Python package for regression. While the focus of sklearn (will be covered later in class) rather lies machine learning applications, statsmodels (as the name suggests) has a rather statitics-oriented focus. We will briefly present the basic functionality of its regression functions by revisiting the Iris data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "df = pd.read_csv (\"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\", names = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"species\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 1: Bivariate Prediction\n",
    "We want to fit a regession model that estimates sepal length from sepal width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify predictors X and target Y\n",
    "Y = df.sepal_length\n",
    "X = df.sepal_width\n",
    "# most importantly: we have to add a constant term to estimate the intercept\n",
    "X = sm.add_constant(X)\n",
    "X[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model: OLS = ordinary least squares\n",
    "model = sm.OLS(y,X)\n",
    "# fit model: only now te model, i.e. the parameters are computed\n",
    "results = model.fit()\n",
    "\n",
    "# print a summary, i.e. an overview on parameters and diagnostics\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get parameters of model, i.e. beta_0 and beta_1\n",
    "params = results.params\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can apply parameters to obtain the predictions of Y based on X\n",
    "np.dot(X,params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unsurprisingly, statsmodels also provides a direct prediction function:\n",
    "results.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 2: Multivariate Regression\n",
    "Now we want to include all other numerical columns from the data to fit to estimate sepal length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# statsmodels also provides a formula syntax, which requires an additional import\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# formula syntax: dependent variable ~ predictor1 + predictor2 +.....\n",
    "# note that intercept is fit automatically\n",
    "model = ols(\"sepal_length ~ sepal_width + petal_width + petal_length\", data=df)\n",
    "\n",
    "results = model.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formula syntax: dependent variable ~ predictor1 + predictor2 +.....\n",
    "# note that intercept is fit automatically\n",
    "\n",
    "# add interaction and squared term\n",
    "model = ols(\"sepal_length ~ sepal_width:petal_width + np.square(petal_length) + sepal_width + petal_width + petal_length\", data=df)\n",
    "\n",
    "results = model.fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Fitting an artificial data set\n",
    "\n",
    "We want to implement OLS regression and test it on artificial data. Thus, in this task you may not yet use the statsmodels functions (except for checking results)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Creating artificial data\n",
    "Create an artificial dataset which consists of:\n",
    "* a vector $x$ consisting of 100 (float) values between 0 and 1\n",
    "* a vector $y = 10x +\\varepsilon$, in which for each element, the error $\\varepsilon_i$ is drawn from the standard normal distribution.\n",
    "Create a scatterplot of x against y!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Implementing OLS regression\n",
    "Write a function that takes as input a numpy vector of target values $y$, and a matrix of predictors $X$, and returns the parameter vector $\\beta$ resulting from OLS regression. Apply this function to fit a model on your artificial data, compute the predictions, and add the resulting regression line to the plot from a). Remember to add a constant term!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Diagnostics 1: The Bootstrap\n",
    "\n",
    "Based on your regression function, write a function that again takes as input a predictor matrix $X$ and a target column $y$, plus a integer $N$, and bootstraps the data $N$ times to compute the 95% confidence interval for each parameter. Specifically, return one parameter vector for the bottom beta values, and one parameter vector for the top values. \n",
    "Apply this function to estimate the confidence intervals on our artificial data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) Diagnostics 2: The $R^2$ score.\n",
    "\n",
    "Write a function that takes as input a ground truth vector y, its prediction y_hat, and computes the $R^2$ value of that prediction! Does your model explain most of the variance in the artificial data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Predicting Student Performance\n",
    "\n",
    "We revisit the student performance dataset from last week's exercise and aim to estimate the exam performance in math. In this task you may use statsmodels!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Data Preprocessing\n",
    "\n",
    "Load the student performance data into a dataframe. Since we want to estimate students performance in math, separate this column from the dataframe. On the remaining columns, transform, i.e. dummy-code all categorical columns as explained in lecture. Further, check for collinearities. If a pair of highly correlated columns (i.e. pearson correlation > 0.9) is given, remove one of these columns from the predictors. Remember to add a constant term afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Learning a simple regression model\n",
    "\n",
    "Apply statsmodels to estimate the exam performance in math from all other columns, without any column interactions. Remember to use a constant term, and properly transform categorical variables. Which significant effects do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Adding interactions\n",
    "Apply statsmodels to fit a regression model that in addition to the previous model further considers an interaction term between the test preparation curse and each of the continuous columns that are left from the preprocessing. Thus, first add these columns to your predictor matrix, and then compute the corresponding model.\n",
    "Does this interaction yield an improvement or rather cause problems?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) Optimizing adjusted R^2\n",
    "\n",
    "Implement a forward selection routine that takes as input a matrix $X$ of predictors, the index of the constant column in $X$, and a target vector $y$, and returns a submatrix of predictors that produces the optimal adjusted R^2 value, plus a vector of the corresponding column indices, the corresponding parameter vector beta, and the optimal adjusted R² value.\n",
    "Apply this function on the predictor matrix from part c). Which predictors are left?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e) Checking residuals\n",
    "Create a residual plot of your final model and give an interpretation of what you observe!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
